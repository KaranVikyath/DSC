{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved in file: E:\\MS\\Semester\\Summer\\RA\\COIL100\\pretrain_model50\\model50.ckpt\n",
      "Var 0: <tf.Variable 'enc_w0:0' shape=(5, 5, 1, 50) dtype=float32>\n",
      "Var 1: <tf.Variable 'Variable:0' shape=(50,) dtype=float32>\n",
      "Var 2: <tf.Variable 'dec_w0:0' shape=(5, 5, 1, 50) dtype=float32>\n",
      "Var 3: <tf.Variable 'Variable_1:0' shape=(1,) dtype=float32>\n",
      "Var 4: <tf.Variable 'Self-Expressive_Layer/Coef:0' shape=(7200, 7200) dtype=float32>\n",
      "INFO:tensorflow:Restoring parameters from E:\\MS\\Semester\\Summer\\RA\\COIL100\\pretrain_model50\\model50.ckpt\n",
      "model restored\n",
      "epoch: 120 cost: 0.00141907\n",
      "experiment: 0 acc: 0.5272\n",
      "0.5272222222222223\n",
      "0.5272222222222223\n",
      "[0.52722222]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import cluster\n",
    "from munkres import Munkres\n",
    "import scipy.io as sio\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.preprocessing import normalize\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "class ConvAE(object):\n",
    "\tdef __init__(self, n_input, kernel_size, n_hidden, reg_const1 = 1.0, reg_const2 = 1.0, reg = None, batch_size = 256,\\\n",
    "\t\tdenoise = False, model_path = None, logs_path = 'E:\\MS\\Semester\\Summer\\RA\\COIL100\\logs'):\t\n",
    "\t#n_hidden is a arrary contains the number of neurals on every layer\n",
    "\t\tself.n_input = n_input\n",
    "\t\tself.n_hidden = n_hidden\n",
    "\t\tself.reg = reg\n",
    "\t\tself.model_path = model_path\t\t\n",
    "\t\tself.kernel_size = kernel_size\t\t\n",
    "\t\tself.iter = 0\n",
    "\t\tself.batch_size = batch_size\n",
    "\t\tweights = self._initialize_weights()\n",
    "\t\t\n",
    "\t\t# model\n",
    "\t\tself.x = tf.compat.v1.placeholder(tf.float32, [None, self.n_input[0], self.n_input[1], 1])\n",
    "\t\tself.learning_rate = tf.compat.v1.placeholder(tf.float32, [])\n",
    "\t\t\n",
    "\t\tif denoise == False:\n",
    "\t\t\tx_input = self.x\n",
    "\t\t\tlatent, shape = self.encoder(x_input, weights)\n",
    "\n",
    "\t\telse:\n",
    "\t\t\tx_input = tf.add(self.x, tf.random.normal(shape=tf.shape(self.x),\n",
    "\t\t\t\t\t\t\t\t\t\t\t   mean = 0,\n",
    "\t\t\t\t\t\t\t\t\t\t\t   stddev = 0.2,\n",
    "\t\t\t\t\t\t\t\t\t\t\t   dtype=tf.float32))\n",
    "\n",
    "\t\t\tlatent,shape = self.encoder(x_input, weights)\n",
    "\t\tself.z_conv = tf.reshape(latent,[batch_size, -1])\t\t\n",
    "\t\tself.z_ssc, Coef = self.selfexpressive_module(batch_size)\t\n",
    "\t\tself.Coef = Coef\t\t\t\t\t\t\n",
    "\t\tlatent_de_ft = tf.reshape(self.z_ssc, tf.shape(latent))\t\t\n",
    "\t\tself.x_r_ft = self.decoder(latent_de_ft, weights, shape)\t\t\n",
    "\t\t\n",
    "\n",
    "\t\tself.saver = tf.compat.v1.train.Saver([v for v in tf.compat.v1.trainable_variables() if not (v.name.startswith(\"Coef\"))]) \n",
    "\t\t\n",
    "\t\twith tf.name_scope(\"Loss\") as scope:\n",
    "\t\t\tself.cost_ssc = 0.5*tf.reduce_sum(tf.pow(tf.subtract(self.z_conv,self.z_ssc), 2))\n",
    "\t\t\tself.recon =  tf.reduce_sum(tf.pow(tf.subtract(self.x_r_ft, self.x), 2.0))\n",
    "\t\t\tself.reg_ssc = tf.reduce_sum(tf.pow(self.Coef,2))\n",
    "\t\t\ttf.compat.v1.summary.scalar(\"self_expressive_loss\", self.cost_ssc)\n",
    "\t\t\ttf.compat.v1.summary.scalar(\"coefficient_lose\", self.reg_ssc)\t\t\t\n",
    "\t\t\tself.loss_ssc = self.cost_ssc*reg_const2 + reg_const1*self.reg_ssc + self.recon\n",
    "\t\t\ttf.compat.v1.summary.scalar(\"Loss_ssc\", self.loss_ssc) \n",
    "\t\tfor i in weights:\n",
    "\t\t\ttf.compat.v1.summary.histogram(i, weights[i])\n",
    "\t\ttf.compat.v1.summary.histogram(\"C\", self.Coef)\n",
    "\t\tself.merged_summary_op = tf.compat.v1.summary.merge_all()\t\t\n",
    "\t\tself.optimizer_ssc = tf.compat.v1.train.AdamOptimizer(learning_rate = self.learning_rate).minimize(self.loss_ssc)\n",
    "\t\tself.init = tf.compat.v1.global_variables_initializer()\n",
    "\t\tself.sess = tf.compat.v1.InteractiveSession()\n",
    "\t\tself.sess.run(self.init)\n",
    "\t\tself.save_model()\n",
    "\t\tfor i, var in enumerate(self.saver._var_list):\n",
    "\t\t\tprint('Var {}: {}'.format(i, var))\n",
    "\t\tself.summary_writer = tf.compat.v1.summary.FileWriter(logs_path, graph=tf.compat.v1.get_default_graph())\n",
    "\n",
    "\tdef _initialize_weights(self):\n",
    "\t\tall_weights = dict()\n",
    "\t\tn_layers = len(self.n_hidden)\n",
    "\n",
    "\n",
    "\t\tall_weights['enc_w0'] = tf.compat.v1.get_variable(\"enc_w0\", shape=[self.kernel_size[0], self.kernel_size[0], 1, self.n_hidden[0]],\n",
    "\t\t\tinitializer=tf.keras.initializers.GlorotNormal(),regularizer = self.reg)\n",
    "\t\t\n",
    "\t\tall_weights['enc_b0'] = tf.Variable(tf.zeros([self.n_hidden[0]], dtype = tf.float32)) # , name = 'enc_b0'\n",
    "\t\t\n",
    "\t\titer_i = 1\n",
    "\t\twhile iter_i < n_layers:\n",
    "\t\t\tenc_name_wi = 'enc_w' + str(iter_i)\n",
    "\t\t\tall_weights[enc_name_wi] = tf.compat.v1.get_variable(enc_name_wi, shape=[self.kernel_size[iter_i], self.kernel_size[iter_i], self.n_hidden[iter_i-1], \\\n",
    "\t\t\t\t\t\tself.n_hidden[iter_i]], initializer=tf.keras.initializers.GlorotNormal(),regularizer = self.reg)\n",
    "\t\t\tenc_name_bi = 'enc_b' + str(iter_i)\n",
    "\t\t\tall_weights[enc_name_bi] = tf.Variable(tf.zeros([self.n_hidden[iter_i]], dtype = tf.float32)) # , name = enc_name_bi\n",
    "\t\t\titer_i = iter_i + 1\n",
    "\t\t\n",
    "\t\t\t\t\n",
    "\t\titer_i = 1\n",
    "\t\twhile iter_i < n_layers:\t\n",
    "\t\t\tdec_name_wi = 'dec_w' + str(iter_i - 1)\n",
    "\t\t\tall_weights[dec_name_wi] = tf.compat.v1.get_variable(dec_name_wi, shape=[self.kernel_size[n_layers-iter_i], self.kernel_size[n_layers-iter_i], \n",
    "\t\t\t\t\t\tself.n_hidden[n_layers-iter_i-1],self.n_hidden[n_layers-iter_i]], initializer=tf.keras.initializers.GlorotNormal(),regularizer = self.reg)\n",
    "\t\t\tdec_name_bi = 'dec_b' + str(iter_i - 1)\n",
    "\t\t\tall_weights[dec_name_bi] = tf.Variable(tf.zeros([self.n_hidden[n_layers-iter_i-1]], dtype = tf.float32)) # , name = dec_name_bi\n",
    "\t\t\titer_i = iter_i + 1\n",
    "\t\t\t\n",
    "\t\tdec_name_wi = 'dec_w' + str(iter_i - 1)\n",
    "\t\tall_weights[dec_name_wi] = tf.compat.v1.get_variable(dec_name_wi, shape=[self.kernel_size[0], self.kernel_size[0],1, self.n_hidden[0]],\n",
    "\t\t\tinitializer=tf.keras.initializers.GlorotNormal(),regularizer = self.reg)\n",
    "\t\tdec_name_bi = 'dec_b' + str(iter_i - 1)\n",
    "\t\tall_weights[dec_name_bi] = tf.Variable(tf.zeros([1], dtype = tf.float32)) # , name = dec_name_bi\n",
    "\t\treturn all_weights\t\n",
    "\t\n",
    "\n",
    "\t# Building the encoder\n",
    "\tdef encoder(self,x, weights):\n",
    "\t\tshapes = []\n",
    "\t\tshapes.append(x.get_shape().as_list())\n",
    "\t\tx_shape = [self.batch_size,self.n_input[0],self.n_input[1],1]\n",
    "\t\twith tf.name_scope(\"Encoder\") as scope:\n",
    "\t\t\t\n",
    "\t\t\tlayeri = tf.nn.bias_add(tf.nn.conv2d(x, filters=weights['enc_w0'], strides=[1,2,2,1],padding='SAME'),weights['enc_b0']) \n",
    "\t\t\tlayeri = tf.nn.relu(layeri)\n",
    "\t\t\tshapes.append(layeri.get_shape().as_list())\n",
    "\t\t\t\n",
    "\t\t\tn_layers = len(self.n_hidden)\n",
    "\t\t\titer_i = 1\n",
    "\t\t\twhile iter_i < n_layers:\n",
    "\t\t\t\tlayeri = tf.nn.bias_add(tf.nn.conv2d(layeri, filters=weights['enc_w' + str(iter_i)], strides=[1,2,2,1],padding='SAME'),weights['enc_b' + str(iter_i)])\n",
    "\t\t\t\tlayeri = tf.nn.relu(layeri)\n",
    "\t\t\t\tshapes.append(layeri.get_shape().as_list())\n",
    "\t\t\t\titer_i = iter_i + 1\n",
    "\t\t\t\n",
    "\t\t\tlayer3 = layeri\n",
    "\t\t\treturn  layer3, shapes\n",
    "\n",
    "\t# Building the decoder\n",
    "\tdef decoder(self,z, weights, shapes):\n",
    "\t\twith tf.name_scope(\"Decoder\") as scope:\n",
    "\t\t\tn_layers = len(self.n_hidden)\t\t\n",
    "\t\t\tlayer3 = z\n",
    "\t\t\titer_i = 0\n",
    "\t\t\twhile iter_i < n_layers:\n",
    "\t\t\t\tshape_de = shapes[n_layers - iter_i - 1] \n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\tlayer3 = tf.add(tf.nn.conv2d_transpose(layer3, weights['dec_w' + str(iter_i)], tf.stack([tf.shape(self.x)[0],shape_de[1],shape_de[2],shape_de[3]]),\\\n",
    "\t\t\t\t\t\tstrides=[1,2,2,1],padding='SAME'), weights['dec_b' + str(iter_i)])\n",
    "\t\t\t\tlayer3 = tf.nn.relu(layer3)\n",
    "\t\t\t\titer_i = iter_i + 1\n",
    "\t\t\treturn layer3\n",
    "\n",
    "\n",
    "\n",
    "\tdef selfexpressive_module(self,batch_size):\n",
    "\t\twith tf.name_scope(\"Self-Expressive_Layer\") as scope:\n",
    "\t\t\tCoef = tf.Variable(1.0e-4 * tf.ones([self.batch_size, self.batch_size],tf.float32), name = 'Coef')\t\t\t\n",
    "\t\t\tz_ssc = tf.matmul(Coef,\tself.z_conv)\n",
    "\t\t\treturn z_ssc, Coef\n",
    "\n",
    "\n",
    "\tdef finetune_fit(self, X, lr):\n",
    "\t\tC,l1_cost, l2_cost, summary, _ = self.sess.run((self.Coef, self.reg_ssc, self.cost_ssc, self.merged_summary_op, self.optimizer_ssc), \\\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tfeed_dict = {self.x: X, self.learning_rate: lr})\n",
    "\t\tself.summary_writer.add_summary(summary, self.iter)\n",
    "\t\tself.iter = self.iter + 1\n",
    "\t\treturn C, l1_cost,l2_cost \n",
    "\t\n",
    "\tdef initlization(self):\n",
    "\t\tself.sess.run(self.init)\t\n",
    "\n",
    "\tdef transform(self, X):\n",
    "\t\treturn self.sess.run(self.z_conv, feed_dict = {self.x:X})\n",
    "\n",
    "\tdef save_model(self):\n",
    "\t\tsave_path = self.saver.save(self.sess,self.model_path)\n",
    "\t\tprint (\"model saved in file: %s\" % save_path)\n",
    "\n",
    "\tdef restore(self):\n",
    "\t\tself.saver.restore(self.sess, self.model_path)\n",
    "\t\tprint (\"model restored\")\n",
    "\n",
    "\n",
    "\n",
    "def best_map(L1,L2):\n",
    "\t#L1 should be the labels and L2 should be the clustering number we got\n",
    "\tLabel1 = np.unique(L1)\n",
    "\tnClass1 = len(Label1)\n",
    "\tLabel2 = np.unique(L2)\n",
    "\tnClass2 = len(Label2)\n",
    "\tnClass = np.maximum(nClass1,nClass2)\n",
    "\tG = np.zeros((nClass,nClass))\n",
    "\tfor i in range(nClass1):\n",
    "\t\tind_cla1 = L1 == Label1[i]\n",
    "\t\tind_cla1 = ind_cla1.astype(float)\n",
    "\t\tfor j in range(nClass2):\n",
    "\t\t\tind_cla2 = L2 == Label2[j]\n",
    "\t\t\tind_cla2 = ind_cla2.astype(float)\n",
    "\t\t\tG[i,j] = np.sum(ind_cla2 * ind_cla1)\n",
    "\tm = Munkres()\n",
    "\tindex = m.compute(-G.T)\n",
    "\tindex = np.array(index)\n",
    "\tc = index[:,1]\n",
    "\tnewL2 = np.zeros(L2.shape)\n",
    "\tfor i in range(nClass2):\n",
    "\t\tnewL2[L2 == Label2[i]] = Label1[c[i]]\n",
    "\treturn newL2\n",
    "\n",
    "def thrC(C,ro):\n",
    "\tif ro < 1:\n",
    "\t\tN = C.shape[1]\n",
    "\t\tCp = np.zeros((N,N))\n",
    "\t\tS = np.abs(np.sort(-np.abs(C),axis=0))\n",
    "\t\tInd = np.argsort(-np.abs(C),axis=0)\n",
    "\t\tfor i in range(N):\n",
    "\t\t\tcL1 = np.sum(S[:,i]).astype(float)\n",
    "\t\t\tstop = False\n",
    "\t\t\tcsum = 0\n",
    "\t\t\tt = 0\n",
    "\t\t\twhile(stop == False):\n",
    "\t\t\t\tcsum = csum + S[t,i]\n",
    "\t\t\t\tif csum > ro*cL1:\n",
    "\t\t\t\t\tstop = True\n",
    "\t\t\t\t\tCp[Ind[0:t+1,i],i] = C[Ind[0:t+1,i],i]\n",
    "\t\t\t\tt = t + 1\n",
    "\telse:\n",
    "\t\tCp = C\n",
    "\n",
    "\treturn Cp\n",
    "\n",
    "def post_proC(C, K, d, alpha):\n",
    "\t# C: coefficient matrix, K: number of clusters, d: dimension of each subspace\n",
    "\tC = 0.5*(C + C.T)\n",
    "\tr = d*K + 1\t\n",
    "\tU, S, _ = svds(C,r,v0 = np.ones(C.shape[0]))\n",
    "\tU = U[:,::-1] \n",
    "\tS = np.sqrt(S[::-1])\n",
    "\tS = np.diag(S)\n",
    "\tU = U.dot(S)\n",
    "\tU = normalize(U, norm='l2', axis = 1)  \n",
    "\tZ = U.dot(U.T)\n",
    "\tZ = Z * (Z>0)\n",
    "\tL = np.abs(Z ** alpha)\n",
    "\tL = L/L.max()\n",
    "\tL = 0.5 * (L + L.T)\t\n",
    "\tspectral = cluster.SpectralClustering(n_clusters=K, eigen_solver='arpack', affinity='precomputed',assign_labels='discretize')\n",
    "\tspectral.fit(L)\n",
    "\tgrp = spectral.fit_predict(L) + 1 \n",
    "\treturn grp, L\n",
    "\n",
    "def err_rate(gt_s, s):\n",
    "\tc_x = best_map(gt_s,s)\n",
    "\terr_x = np.sum(gt_s[:] != c_x[:])\n",
    "\tmissrate = err_x.astype(float) / (gt_s.shape[0])\n",
    "\treturn missrate  \n",
    "\n",
    "\n",
    "# main function starts here\n",
    "\n",
    "data = sio.loadmat('E:\\MS\\Semester\\Summer\\RA\\COIL100\\COIL100.mat')\n",
    "Img = data['fea']\n",
    "Label = data['gnd']\n",
    "Img = np.reshape(Img,(Img.shape[0],32,32,1))\n",
    "\n",
    "n_input = [32,32]\n",
    "kernel_size = [5]\n",
    "n_hidden = [50]\n",
    "batch_size = 7200\n",
    "\n",
    "model_path = 'E:\\MS\\Semester\\Summer\\RA\\COIL100\\pretrain_model50\\model50.ckpt'\n",
    "ft_path = 'E:\\MS\\Semester\\Summer\\RA\\COIL100\\pretrain_model50\\model50.ckpt'\n",
    "logs_path = 'E:\\MS\\Semester\\Summer\\RA\\COIL100\\logs'\n",
    "\n",
    "_index_in_epoch = 0\n",
    "_epochs= 0\n",
    "num_class = 100 #how many class we sample\n",
    "num_sa = 72\n",
    "batch_size_test = num_sa * num_class\n",
    "\n",
    "\n",
    "iter_ft = 0\n",
    "ft_times = 120\n",
    "display_step = ft_times\n",
    "alpha = 0.04\n",
    "learning_rate = 1e-3\n",
    "\n",
    "reg1 = 1.0\n",
    "reg2 = 30.0\n",
    "\n",
    "\n",
    "CAE = ConvAE(n_input = n_input, n_hidden = n_hidden, reg_const1 = reg1, reg_const2 = reg2, kernel_size = kernel_size, \\\n",
    "\t\t\tbatch_size = batch_size_test, model_path = model_path, logs_path= logs_path)\n",
    "\n",
    "acc_= []\n",
    "for i in range(0,1):\n",
    "\tcoil100_all_subjs = np.array(Img[i*num_sa:(i+num_class)*num_sa,:])\n",
    "\tcoil100_all_subjs = coil100_all_subjs.astype(float)\t\n",
    "\tlabel_all_subjs = np.array(Label[i*num_sa:(i+num_class)*num_sa])    \n",
    "\tlabel_all_subjs = label_all_subjs - label_all_subjs.min() + 1    \n",
    "\tlabel_all_subjs = np.squeeze(label_all_subjs)  \t\n",
    "\n",
    "\tCAE.initlization()\n",
    "\tCAE.restore()\n",
    "\tZ = CAE.transform(coil100_all_subjs)\t\n",
    "\tfor iter_ft  in range(ft_times):\n",
    "\t\titer_ft = iter_ft+1\n",
    "\t\tC,l1_cost,l2_cost = CAE.finetune_fit(coil100_all_subjs,learning_rate)\n",
    "\t\tif (iter_ft % display_step == 0) and (iter_ft >= 50):\n",
    "\t\t\tprint (\"epoch: %.1d\" % iter_ft, \"cost: %.8f\" % (l1_cost/float(batch_size_test)))\n",
    "\t\t\tC = thrC(C,alpha)\t\t\t\n",
    "\t\t\ty_x, CKSym_x = post_proC(C, num_class, 12 , 8)\t\t\t\n",
    "\t\t\tmissrate_x = err_rate(label_all_subjs,y_x)\t\t\t\n",
    "\t\t\tacc = 1 - missrate_x\n",
    "\t\t\tprint (\"experiment: %d\" % i,\"acc: %.4f\" % acc)\n",
    "\tacc_.append(acc)\n",
    "\t\n",
    "acc_ = np.array(acc_)\n",
    "m = np.mean(acc_)\n",
    "me = np.median(acc_)\n",
    "print(m)\n",
    "print(me)\n",
    "print(acc_)\n",
    "\n",
    "\t\t\n",
    "\t\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
